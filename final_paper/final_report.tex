\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{color}
\usepackage{makeidx}
\usepackage{titlepic}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[numbers]{natbib}

\title{On-line Option Discovery and De-Aliasing for Direct Policy Search}
\date{July 4, 2013}
\author{Laura Vogelaar}
\titlepic{\includegraphics{cover.png}}

\makeindex

\newcommand{\polg}   {\ensuremath{\Pi}}
\newcommand{\pol}    {\ensuremath{\pi}}
\newcommand{\act}    {\ensuremath{\mathbf{a}}}
\newcommand{\sta}    {\ensuremath{\mathbf{s}}}
\newcommand{\appsp}  {\ensuremath{\Theta}}
\newcommand{\feat}   {\ensuremath{f}}
\newcommand{\featsp} {\ensuremath{\emph{F}}}
\newcommand{\costsp} {\ensuremath{R}}
\newcommand{\taskp}  {\ensuremath{\mathbf{q}}}
\newcommand{\taskpv} {\ensuremath{q}}
\newcommand{\costf}  {\ensuremath{J}}
\newcommand{\obsm}   {\ensuremath{O}}
\newcommand{\app}    {\ensuremath{\bm{\theta}}}
\newcommand{\taskpsp}{\ensuremath{Q}}
\newcommand{\featv}  {\ensuremath{\mathbf{f}}}
\newcommand{\covar}  {\ensuremath{\bm{\Sigma}}}

\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}

\begin{document}

\maketitle

\newpage
\thispagestyle{empty}
\mbox{}
\newpage
\vspace*{\fill}
{\color{red}
\textbf{\centerline{Notice of non-confidentiality:}}
{This document is not confidential. It can therefore be communicated externally in printed or electronic format.}
\newline
\newline
\textbf{\centerline{Note de non confidentialit\'{e}:}}
{Le document est non confidentiel. Il peut donc \^{e}tre communiqu\'{e} \`{a} l'ext\'{e}rieur sous format papier mais \'{e}galement diffus\'{e} sous format \'{e}lectronique.}
}
\vspace*{\fill}

\newpage
\vspace*{\fill}
\centerline{\textbf{Special Thanks}}
Special thank you to Freek Stulp for his mentorship and guidance throughout this internship experience.
\vspace*{\fill}
\newpage
\begin{abstract}
This is a report of a research internship performed at ENSTA ParisTech in the Robotics and Computer Vision Lab. The research goal was to design and determine the feasibility of a method for a robot to automatically distinguish when multiple tasks are being given and learn to perform them. The task used was for a simulated robot to lift various objects. At first the robot treated all perceived contexts as the same - known as perceptual aliasing. Then, based on given features of these objects and a measure of how successful the robot was after each action it tried, it created a mapping from perception to action, while at the same time optimizing all the actions. This method was successful when the actions for each task were similar and was not successful when the actions were very different. In conclusion, online de-aliasing uses a local search, but with a local method it is not possible to perform global de-aliasing.
\end{abstract}
\newpage
\tableofcontents
\newpage
\section{Introduction}

This internship was completed at \'{E}cole Nationale Sup\'{e}rieure de Techniques Avanc\'{e}es, Robotics and Computer Vision Lab under the supervision of Dr. Freek Stulp. The internship consisted of a research project, whose topic was how to autonomously de-alias perceptually-aliased tasks while running stochastic optimization. Dr. Stulp has been using stochastic optimization already in a variety of applications \cite{stulp12reinforcement}, and my goal was to add a way to create context-specific optimizers in instances where there are distinctly different contexts and actions. I worked directly under Dr. Stulp, and regular meetings between the two of us facilitated the development and direction of the project.

One of the longstanding difficulties in robotics is autonomously learning new skills to conquer different tasks. A human can distinguish that when presented with one particular situation, the best action may be different than when presented with a different situation. Take the example of lifting a translucent milk carton. We've all experienced picking up an empty carton when we expected it to be full and suffering from overshooting. This shows that there are two different actions that are performed: one action for when the carton is empty, and one action for when the carton is full. It is only from our experience in lifting milk cartons that we know the fullness is the important feature to decide our action based upon, not the color or the type of milk or any number of other irrelevant features. This mapping of relevant features to actions is called a ``perceptual class'' \cite{piater11learning}. Imagine now the case where the milk carton is opaque instead of translucent. There are still two actions that should be taken, one when the carton is full, and one when the carton is empty, but we can no longer distinguish the two perceptual classes, since there are no relevant features. This is known as perceptual aliasing. When perceptual aliasing is present, the best performance will either be an action that is a compromise in-between the two ideal actions that will work partially for both tasks, or one of the ideal actions that will work well for one task and poorly for the other task. 

The goal of this work is to start in a situation where we don't know that the fullness is what matters for picking up the milk carton, and we don't know that there are two tasks --- we start assuming there is only one task --- and to discover that there are two tasks, that they are defined by the fullness/emptiness, and what the ideal action is to take in both cases. This process is called perceptual de-aliasing since at first all perceptual classes are aliased into one task, and the algorithm will split the perceptual class into non-aliased classes \cite{daniel12learning}. We will assume that there is at least one feature that can be used to detect each task (there is no opaque carton). The motivation is to perform perceptual de-aliasing online, while also optimizing the action choices at the same time.

The experiments that were performed applied stochastic optimization and a perceptual de-aliasing technique to manipulation. The different tasks were different objects that were to be lifted by a robot arm with a simple two finger gripper. The stochastic optimization algorithm and perceptual de-aliasing techniques will be discussed in further detail in the methods section.


\section{Formalization}
%policy

\subsection{Parameterized Policies}

\begin{align}
\label{eq:policy}\pi(\act|\sta,\app) &\equiv  \pi_{\app}(\act|\sta) & \app \in \appsp \\
\appsp &\in \mathbb{R}^N & \mbox{Action space}
\end{align}

Eq \ref{eq:policy} defines a policy which maps states to actions and is parameterized by $\app$. The action space is the parameters needed to complete the action. It can be the goal position, or the weights of a Dynamic Movement Primitive (DMP).

\subsubsection{Policy Improvement through Stochastic Optimization}

\begin{align}
\costf\mbox{: } &\label{eq:cost} \appsp \mapsto \mathbb{R}&\mbox{Cost function}
\end{align}

Policy improvement: find \app that minimizes \costf.

Stochastic optimization general method for solving black-box opimization problems. BBO: nothing is known about \costf. One approach: treat policy improvement as BBO, apply stochastic optimization.

When the parameter $\app$ in Eq. \ref{eq:policy} is unknown or needs to be optimized, it can be represented instead using a mean and variance matrix to give a normal distribution for the value of $\app$. The policy parameter vector is then chosen from a normal distribution with the mean $\app_\mu$ and the variance $\covar$ shown in Eq. \ref{eq:opt}.

\color{red}Explain stochastic optimization here \color{black}


\begin{align*}
\app_{k=1\dots K} & {\sim}\mathcal{N}({\app_\mu},{\covar})\\
\forall k~J_k & = J(\app_k)\\
\forall k~P_k & = \mbox{``lower cost} \Rightarrow \mbox{higher weight''}\\
\app^{new} & = \sum_{k=1}^{K} P_k \app_k\\
\covar^{new} & = \sum_{k=1}^{K}P_k(\app_k-\app)(\app_k-\app)^\intercal
\end{align*}

Instead of having the parameters $\app_\mu$ and $\covar$ as parameters of the optimization algorithm, as is customary, these parameters are stored in the policy itself. 

\begin{align}
\label{eq:opt}\pi(\act|\sta,\app_\mu,\covar)
\end{align}



\subsection{Parameterized Policies}

There arise many situations where the  policy parameter vector $\app$ is dependent on the context. The context is the set of relevant task parameters $\taskp$ which have an influence on the values of the policy parameters. Task parameters, \emph{\taskpsp}, define a task. In the case of application to manipulation, these parameters could be the position and orientation of the object, the object's geometry, the object's color, and any number of other defining properties. These task parameters are constant in time and do not include any robot properties. 

\begin{align}
\label{eq:gating}\polg(\app|\taskp)
\end{align}

To define the relationship between task parameters and policy parameters, a new policy is needed which will map $\taskp$ to $\app$, called the gating policy (Eq. \ref{eq:gating}). The parameters $\app$ it controls are inputs to the action policy, which controls the actual action being performed. Now there is a two-level system: input task parameters into the gating policy which outputs policy parameters for the action policy, input these policy parameters to the action policy to perform the action. 


Parameterized policies use regression techniques to map the relevant task parameter vector $\taskp$ to the chosen policy parameter vector $\app$.

\color{red} Explain Gennaro's work here \color{black}

\begin{align}
\costf\mbox{: } &\label{eq:cost_task} \appsp \times \taskpsp \mapsto \mathbb{R}&\mbox{Cost function}
\end{align}



%\newcommand{\polg}   {\ensuremath{\pi^g}}
%\newcommand{\pol}    {\ensuremath{\pi}}
%\newcommand{\act}    {\ensuremath{\mathbf{a}}}
%\newcommand{\sta}    {\ensuremath{\mathbf{s}}}
%\newcommand{\appsp}  {\ensuremath{\Theta}}
%\newcommand{\feat}   {\ensuremath{f}}
%\newcommand{\featsp} {\ensuremath{\emph{F}}}
%\newcommand{\costsp} {\ensuremath{R}}
%\newcommand{\taskp}  {\ensuremath{\mathbf{q}}}
%\newcommand{\taskpv} {\ensuremath{q}}
%\newcommand{\costf}  {\ensuremath{J}}
%\newcommand{\obsm}   {\ensuremath{O}}
%\newcommand{\app}    {\ensuremath{\bm{\theta}}}
%\newcommand{\taskpsp}{\ensuremath{Q}}
%\newcommand{\featv}  {\ensuremath{\mathbf{f}}}
%\newcommand{\covar}  {\ensuremath{\bm{\Sigma}}}

\begin{table}[ht]
  \centering
\begin{scriptsize}
\begin{tabular}{|l|l|rl|rl|rl|}
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\hline
\multicolumn{8}{|l|}{\em Parameterized skills: Two-step methods}\\
%-------------------------------------------------------------------------
This report                          
& $\pol(\act|\sta,\app)$ 
& {\bf task pars}            & \taskp 
& {\bf policy pars}          & \app 
& {\bf policy pars function} & $\polg: \taskpsp \mapsto \appsp$ ($\polg: \taskp \mapsto \app$)\\  
%-------------------------------------------------------------------------
\citet{ude10taskspecific}      
& not specified
& task pars            & $\mathbf{q}$ 
& policy pars          & $[\mathbf{w},\tau,g]$ 
& function             & $\mathbf{G}: \mathbf{q} \mapsto [\mathbf{w},\tau,g]$ \\  
%-------------------------------------------------------------------------
\citet{kupcsik13dataefficient} 
& $\pi(\mathbf{u}|\mathbf{x},\mathbf{\omega})$ 
& context              & $\mathbf{s}$
& policy pars          & $\mathbf{\omega}$ 
& upper level policy   & $\pi(\mathbf{\omega}|\mathbf{s})$ \\  
%-------------------------------------------------------------------------
\citet{silva12learning}        
& $\pi_\mathbf{\theta}$
& task pars            & $\mathbf{\tau}$ 
& policy pars          & ${\theta}$
& parameterized skill  & $\Theta: T \mapsto \mathbb{R}^N$ ($\Theta: \mathbf{\tau} \mapsto \mathbf{\theta}$)\\
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\hline
\multicolumn{8}{|l|}{\em Parameterized skills: One-step methods}\\
%-------------------------------------------------------------------------
\citet{matsubara11learning}    
& $f(x,\mathbf{s}; \mathbf{W})$
& style pars           & $\mathbf{s}$ 
& \multicolumn{2}{c|}{N.A.}   
& \multicolumn{2}{c|}{N.A.} \\  
%-------------------------------------------------------------------------
\citet{stulp13learning}        
& $h_\mathbf{u}(s,\mathbf{q})$
& task pars            & $\mathbf{q}$ 
& \multicolumn{2}{c|}{N.A.}   
& \multicolumn{2}{c|}{N.A.} \\  
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\hline
\multicolumn{8}{|l|}{\em Option policies}\\
%-------------------------------------------------------------------------
\citet{daniel12learning}    
& $\pi(\mathbf{a}|\mathbf{s},o)$
& \multicolumn{2}{c|}{N.A. ($\approx\mathbf{s}$)}   
& option           & $o$ 
& gating network   & $\pi(o|\mathbf{s})$ \\  
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\end{tabular}
\end{scriptsize}
  \caption{\label{tab_nomenclature} Different nomenclature in related work on parameterized skills with trajectory-based policy representations.}
\end{table}

\subsubsection{Stochastic Optimization for Parameterized Policies}

\begin{align}
\label{eq:gating_par}\polg(\app_\mu,\covar|\taskp)
\end{align}

\citet{kupcsik13dataefficient}

\citet{marin12towards}



\subsection{Option Policies}

When there are a discrete number of options for policy parameters, classification is used instead of regression for the gating policy. \citet{daniel12learning} ran experiments using this two-level policy structure with the goal of finding multiple action policy solutions to a single task. The task parameters used corresponded directly with which option to take. 

\subsubsection{Stochastic Optimization for Option Policies}

Basic idea: use data gathered during stochastic optimization to discover option policies

This is your work. See later.


\section{Perceptual De-aliasing}

\color{red}Combine the use of a gating policy (decision trees) and an action policy that uses stochastic optimization. Get the data from the stochastic optimization updates. Also we use features not q. Cite \citet{piater11learning}\color{black}

We don't always have direct excess to the task parameters. Instead, we have access to the detectable features which may or may not be relevant in determining the action policy parameters.  The feature space contains the set of all possible feature vectors that can be seen. Each feature within this vector can be either a real number or one of a discrete set. In the robot example, the features might be the color of the object, in which case its value is one of a discrete set such as \{red, yellow, blue\}, or it could be the color intensity measured by a sensor, in which case it would be a continuous numeric value. The features which are observed are a function of the task parameters \eqref{eq:obs}. The color sensor value would be a function of the object's color, and the visual data would be a function of the object's geometry and orientation with respect to the camera, and so on.

\begin{align}
\obsm\mbox{: } &\label{eq:obs}\taskpsp \mapsto \featsp& \mbox{Observation model}\\
\featsp &= \featsp_1 \times \featsp_2 \times ... \times \featsp_N & \mbox{Feature space}\\
\featsp_i &\in \mathbb{R}, or \featsp_i \in \{C_1,C_2,...,C_N\}&\notag\\
\featv &\in \featsp, with \featv = <\feat_1,\feat_2,...,\feat_n>&
\end{align}

\begin{align}
\label{eq:fgating}\pi(\app,\feat)
\end{align}

It is necessary to define the idea of relevance: what makes a feature or a task parameter relevant? A task parameter is relevant when it can take on different values that will lead to different optimal actions \eqref{eq:relq}. We are assuming that there is a task parameter that independently influences the cost. It is possible that a case could arise where only changing two cost parameters at once causes a different action to be taken, but we are not considering that at this time. 

\begin{align}
\taskp_i =& <\taskpv_1,\taskpv_2,...,\taskpv_i,...\taskpv_N> \notag & 
\\
\taskp'_i =& <\taskpv_1,\taskpv_2,...,\taskpv'_i,...\taskpv_N> \notag & \taskpv_i \neq \taskpv'_i \\
\label{eq:relq} \taskp_i \in \taskpsp_{REL} \iff&\argmin{\app}\costf(\app,\taskp_i) \neq \argmin{\app}\costf(\app,\taskp'_i)\\
\taskpsp_{REL} \cup & \taskpsp_{IRR} = \taskpsp,   \taskpsp_{REL} \cap \taskpsp_{IRR} = \emptyset 
\end{align}

Multiple tasks are present when \eqref{eq:relq} is true since there are two ideal actions. Therefore there are multiple tasks present if and only if there is at least one relevant task parameter. If a difference in one or more of the features can be detected when the task parameter changes, then it will be possible to perceptually de-alias the two tasks. Since the goal is to de-alias in an online fashion, the ideal action, $\argmin{\app}\costf(\app,\taskp)$, will not be known at the moment of de-aliasing. Instead, the value of the cost function $\costf$ is used as an indicator by how different it is for different values of the task parameter in question \eqref{eq:relq} as to whether or not the ideal action will be different in the two cases. Sometimes this will lead to overzealous de-aliasing, but perceptual classes can easily be merged again if it is discovered the same actions are being performed.

\section{Methods}

\subsection{Task Splitting}
The following description will be given in the context of the simulated robot which will be discussed further later in the paper, but this algorithm can be equally well applied to performing and action which is defined by a discrete number of parameters. 

The robot is given an initial starting motion through imitation learning. Then the robot will choose K sample actions, in which each action parameter is taken from a normal distribution centered around the imitation learning value and with a predefined initial covariance matrix. When the robot is next required to perform a task, it will record the features present, perform a sample action from the distribution, and record the cost that it incurs. Now after each performed task, the robot will accumulate a roll-out which contains the perception, action parameters, and cost. After K tasks have been seen, the sampling distribution and sampling covariance matrix are updated, biased towards the lower cost actions. Now future action will be drawn from this new sampling distribution. 

If there is only one task, the algorithm will proceed in this manner, performing stochastic optimization until the local minimum of the cost is found, presumably towards perfecting the demonstrated action. If there are two tasks being shown, which have similar but different actions that need to be performed then this algorithm by itself will never find the context-specific best actions. Therefore, task splitting needs to be introduced in order to detect the difference between two different tasks. After this distinction is made, the two tasks can be treated separately, each with its own sampling distribution. To implement this idea, after K samples are taken, but before the new distribution is updated, a splitting test is run to determine if there are two different tasks that are falsely being treated as the same task.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{one_vs_two_tasks.png}
  \caption{One versus two tasks}
  \label{fig:1vs2tasks}
\end{figure} 

In Figure \ref{fig:1vs2tasks}, the top two plots show the situation where there is only one task, i.e. $\taskpsp_1 = \taskpsp$. The bottom two plots shows the situation where there are two tasks. The blue line shows $\costf(\app,\taskp), \taskp \in \taskpsp_1$

\color{red}Each task has its own cost function. When two tasks are being treated as the same task, some of the samples will be the cost function from one task, and some will be the cost function from the other task. \color{black} When the sampling distribution is at a location where the two cost functions vary, a histogram of the costs will reveal that there are two different clusters. By looking only at the cost distribution, regardless of the action taken, we are assuming that the action taken does not matter. In the case where the covariance matrix for the sampling distribution is small, this serves as a fairly accurate estimation, since all the actions taken in the K samples are very similar. Once the costs are clustered, any supervised learning algorithm can be used to determine which features indicate which cluster. For this work, a classification tree was used. If we had followed in a similar thread as~\citet{piater11learning}, the parallel would have been to determine visual classes by using the covariance matrix. If the covariance matrix remained large, or moved back and forth repeatedly, we could assume this was due to visual class aliasing, but it may also happen by chance, and is dependent on the reinforcement learning algorithm. Instead, by using approximately equivalent actions as if they were the same action, visual classes can be found completely independently of the reinforcement algorithm used. 

Once a new visual class is identified by the supervised learning algorithm, the sampling distribution and covariance is initialized as the sampling distribution of both tasks before the split. If the cost function for both tasks is smooth and has a non-zero gradient at all points, the new optimizer should be able to find the minimum for the new task. However, in real applications, even if the cost function is smooth, there is often only a small range of action parameters near the solution which give any useful feedback. Still using a Gaussian cost function, but this time with a cut-off of two sigma, and in 2D, the cost function could be one of the following two cases. Either there is a large overlap between the non-constant regions of the two cost functions, or there is no overlap between the non-constant regions. In the case of overlap, it is feasible that once splitting occurs, some of the samples of the second distribution will land in a non-constant cost region, thus leading the skill towards the solution. In the case where there is no overlap, the probability of a sample getting a non-constant cost is very small. Without some variability in the costs, the skill will move in a random direction and due to the covariance update methods, will eventually stop, and miss the solution.
 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{cost_function.png}
  \caption{ Possible cost functions for two tasks. Left: likely to find both optimal actions due to overlap. Right: unlikely to find both optimal actions due to large distance between the two optimal actions.}
  \label{fig:costfunction}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{sigma_vs_covar.png}
  \caption{\label{fig_sigma_vs_covar} Success rate of finding two ideal actions, when the cost function is non-constant within a range of 2 Sigma, and the samples are taken from a sampling distribution with the the shown covariance. }
  \label{fig:sigmavscovar}
\end{figure}

Figure \ref{fig:costfunction} shows two examples of this experiment, one in which we would expect both minima to be found (left), and the other (right) which it is unlikely the second minima will be found. The dark circles indicate low-cost areas, and the white background is a constant high-cost area. Splitting will only occur when there is a difference in the cost values of the two tasks for the current action. This means splitting cannot occur in the white portion of the graphs, it can only occur in non-equivalent regions. Therefore, once split, there will be a large gap between the current action and the ideal action for one of the tasks. Figure \ref{fig:sigmavscovar} shows the percentage that both minima are successfully found when varying the sampling covariance and the cost distribution's standard deviation. The likelihood of finding the second skill diminishes when the cost distribution's standard deviation is reduced because the probability of drawing a sample within the 2-sigma radius funnel is lower. In a real life problem, the dimensionality will be very high, the funnel mouth very small, and the search space very large. It is logical to conclude that unless the two skills are very close in all dimensions of the action space, that the probability of finding the second skill is negligible. This conclusion is supported by the results of my experiment.


\subsection{Task Splitting Techniques}

Once a sufficient number of similar actions were taken, the percept-action-cost data was collected. Since the action was very similar, it could be taken as constant, reducing the complexity to percept-cost. Treating each perceived feature individually yielded a two dimensional work space of feature value vs. cost. 

\emph{2D Gaussian Mixture Model}\newline

The two dimensions of feature and cost were plotted against each other and then fitted with a Gaussian Mixture Model [4]. This was done in an attempt to see if there were two Gaussians, and whether they were very different in cost-space and percept-space. Upon further reflection, this only works with truly continuous features, and not with binary or classification features, since in this case, the `value' given in the feature space is meaningless as far as its spacial relation to its neighbors.\newline\newline
\emph{Cluster Costs}\newline

In this technique, the one-dimensional cost vector is clustered in order to see if there are two separate cost functions being seen. If the clusters are significantly different from one another (determined by a Student T-test), then a classification tree is used to find the feature combination that leads to the cluster membership function. Since relevant features are assumed in this experiment, the classification tree is pruned to one layer and then tested to see if it still correctly predicts the membership function to within a defined threshold. If it does, then the feature used is relevant and defines two visual classes with different cost functions. If it does not, then the clustering of the costs indicates that there may be two different cost functions, but that with the given features, it is impossible to distinguish the two tasks. 

The technique of fitting 2D Gaussian distributions is only applicable to situations where the feature is continuous. In Figure \ref{fig:2Dgaussians}, several idealized situations were subjected to this splitting method. The green plots show situations in which a split should ideally be detected, and the red are cases were a split should not be detected. The headings c-clus and f-clus, refer to whether the costs are clusterable and whether the feature itself is clusterable, respectively. When there is a green vertical line on the plot, this indicates that the algorithm decided to split this data, and the location of this line on the x-axis is the cutoff point for the feature that was chosen. For the binary features, 2D Gaussian fitting is not necessary since the variance in the feature space will be zero, thus these situations are decided using a paired Student T-Test with different variances. Figure \ref{fig:clustercosts} shows the results of the clustering algorithm performed on the same set of idealized data. This time the cost must be clusterable in order to find a split, but it will work on categorical data and is more robust. 


\begin{figure}[ht]
  \centerline{
  \includegraphics[width=1.5\columnwidth]{2d_gaussians.png}}
  \caption{\label{fig_2d_gaussians} 2D Gaussian Mixture Model performance on idealized data}
  \label{fig:2Dgaussians}
\end{figure}

\begin{figure}[ht]
  \centerline{
  \includegraphics[width=1.5\columnwidth]{cluster_costs.png}}
  \caption{\label{fig_cluster_costs} Cost Clustering performance on idealized data}
  \label{fig:clustercosts}
\end{figure}

\section{Results}
\subsection{Robot Arm Simulations}
\subsubsection{Experiment Setup}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{robots.png}
  \caption{\label{fig_robots} Left: tall standing rod. Center: short standing rod. Right: short laying-down rod.
  \label{fig:robots}
  }
\end{figure}

The robot simulator V-REP was used to simulate a robot arm with a simple two finger, one degree of freedom gripper. The action space was six-dimensional, and controlled the end position and orientation of the gripper. After the desired end position was reached, the gripper closed, and the arm lifted a predefined amount. The task was to lift an object that was placed before the robot always at the same coordinates. The cost of a trial was given by the weighted combination of how high the object was lifted, how little the orientation of the object changed throughout the lifting, and the minimum distance between the gripper and the surface of the object through the entire motion. The last part of this cost function was added to help compensate for the large number of actions that would have had a constant high cost due to the robot not coming into contact with the object at all. Adding this component allowed for the robot to naturally move closer to the object, until the orientation or lifting success, which were weighted higher, made the part of the cost due to distance negligible. 

	The following pairs of tasks were given to the robot to be solved:
	
Setup 1:	Tall standing rod	vs.	Short standing rod

Setup 2:	Short standing rod	vs. 	Short laying-down rod

	Initially both tasks were treated as if they were one visual class. The initial action distribution was given manually as a successful grasp for one of the two tasks. Once aliasing was detected, the two visual classes were defined by one of the perception variables and optimized independently.
	
Successful grips between the tall standing rod and the short standing rod are very similar. The action parameter is different in only one dimension, and not very far within this dimension. Therefore this situation is more like that discussed on the left half of Figure \ref{fig:costfunction}, where there is cost information for both tasks that overlaps. For this reason, the two skills were successfully found and optimized. In the case of the laying-down rod, the action parameters were similar in three dimensions, but very different in three other dimensions. A different pose was needed to grasp the rod from above instead of from the side since it was lying down. This situation corresponds to the right half of Figure \ref{fig:costfunction}, where the cost information for the two tasks is distinct and far away, making it impossible to use the local aliased solution to find the very different global solution.

\subsubsection{Experiment Results}

\begin{figure}[ht]
  \centerline{
  \includegraphics[width=0.9\columnwidth]{6d.png}}
  \caption{\label{fig_6d} Successful policy parameters divided by task. Red is tall standing rod, blue is short standing rod, and green is short laying-down rod.}
  \label{fig:6d}
\end{figure}

Setup 1 resulted in a split based on the height of the rod. The final action for the tall standing rod was to lift from the center of mass, and the final action for the short standing rod was the lift from just below the top of the rod. Both moves were successful and yielded low costs. Setup 2 also resulted in a split. The final action for the short standing rod was the same as in Setup 1, but the final action for the laying-down rod was an unsuccessful grip similar to that of the short standing rod. What happened in Setup 2 was that the two tasks were correctly identified by a percept variable and split accordingly, but the two tasks required such different actions to be successful that the gap could not be overcome. To further demonstrate this idea, refer to Figure \ref{fig:6d}, which shows the successful trials of each of the three tasks being performed. The tall standing rod is shown in red, the short standing rod in blue, and the short laying-down rod in green. Since this task is 6-dimensional, it is not easy to show all dimensions at once. Instead, on the left are the \emph{x},\emph{y}, and \emph{z} linear dimensions, and on the right are the angular dimensions, \emph{$\alpha$},\emph{$\beta$}, and \emph{$\gamma$}. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\columnwidth]{distance.png}
  \caption{\label{fig_distance} Comparison of distances between the short standing rod task and the other two tasks in action space.}
  \label{fig:distance}
\end{figure}

\newpage
While the three tasks seem quite close to one another in linear space, it is clear that the laying-down rod is very distant from the other two tasks in angular space. This distance explains why the correct action was found between the two standing rods, but not between a standing and a laying-down rod. Since the short standing rod was used as the initial imitation-learned solution, we can measure the likelihood that a sample taken from here will reach a successful grasp for one of the other tasks. One hundred samples were taken with a reasonable covariance matrix and the initial mean of the short standing rod. The Euclidean distance between these samples and the two other tasks was then calculated. In Figure \ref{fig:distance} these distances are plotted, with the 2 sigma cutoff around each task's optimal action. The very tall rod successfully has many samples within range of its optimal action. The laying-down rod does not have any samples anywhere close to being within range. The optimal actions are too different from each other that one solution will not provide any information about the other solution.

\subsection{Global Optima with Local Search}


\section{Related Work}

\citet{daniel12learning} described an approach at IROS 2012  that used relative entropy policy search with hierarchical policy representations. They are not exactly using perceptual classes, but are finding multiple solutions to a single task. For their algorithm, a number of options are predefined, greater than the number of possible optima, and redundant options are deleted as the algorithm runs, with care taken to avoid premature deletion. Their robot playing tetherball example uses a 29-dimensional action space $\appsp$, and imitation learning is used to set the initial parameters of the dynamic motion primitive (DMP). It is not clear if these options all have the same initial policy, or if they are seeded differently. By starting with a large number of redundant options, it is more likely that they will cover the action space and find the possible solutions than if using a single option and then splitting into new options. While this may be feasible for a small number of basis functions, when it is a larger search space, the number of needed initial perceptual classes will increase rapidly. 

\citet{calinon13compliant} uses a Gaussian Mixture Model to estimate the structure of the cost function \costf, mapping the cost as a function of action as if it were a probability density function. In their experiment, only one task was shown, so accordingly there were no task parameters, and the cost function was reduced to $\costf(\app)$ instead of the full $\costf(\app,\taskp)$. Their experiment uses an action space that is 2-dimensional. In very large and high-dimensional action spaces, it will require a very large number of samples to accurately estimate the cost function's Gaussian Mixture Model. To tackle the problem of finding global optimums with a local search, \citet{calinon13compliant} employ decaying exploration noise. If the exploration noise is too low or zero, the optimizer would become trapped in the local minima, and not explore the rest of the cost function.

Task-specific representations were explored by~\cite{piater11learning} in which perception-action solutions were created by interleaving a percept classifier with a reinforcement learner. Effectively this means that while the reinforcement learner is trying to optimize the task at hand, the percept classifier is trying to parse out the perceptual classes. Once the perceptual classes are distinguished, the reinforcement learner will work independently on each of the newly defined perceptual classes. The reinforcement method they used was Markov Decision Processes (MDP). Their method for de-aliasing was to determine if the ideal action $\app$ was converging or not. They measured this convergence by seeing if the difference errors between subsequent iterations of the MDP updates were getting smaller or not. This method is the logical parallel to the Mean Divergence technique I tested. They assume any failure to converge directly is caused by aliasing, and not due to possible inherent instability in the system. They concede that their experiments were run in very small action-spaces $\appsp$, and that scalability may not be feasible. 


\section{Conclusion}

Online de-aliasing will work when the actions that need to be taken in different contexts are near to each other in action-space. This supports the fact that if the robot is trying to learn two movements that are very different, learning one of them does not give any information on how to do the other. Clustering the costs after a number of similar actions successfully allows the determination of visual classes and provides the advantage of not requiring a bond between the chosen reinforcement algorithm (in this case stochastic optimization) and the chosen supervised learning algorithm (in this case decision trees). In the situations where online de-aliasing works, the actions are very similar and based on the perceptual context. This occurs usually when the action is parameterized by the relevant perceptual feature. Future work would be to combine discretized visual classes that can be formed by online de-aliasing into perceptually parameterized skills.

%\bibliographystyle{plain}
\bibliographystyle{plainnat}
\bibliography{final_report}


\end{document}
